{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## IMPORT","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/giankev/PDLPR-algorithm.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:01:53.720951Z","iopub.execute_input":"2025-07-21T14:01:53.721183Z","iopub.status.idle":"2025-07-21T14:02:00.088878Z","shell.execute_reply.started":"2025-07-21T14:01:53.721155Z","shell.execute_reply":"2025-07-21T14:02:00.088097Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PDLPR-algorithm'...\nremote: Enumerating objects: 721, done.\u001b[K\nremote: Counting objects: 100% (83/83), done.\u001b[K\nremote: Compressing objects: 100% (58/58), done.\u001b[K\nremote: Total 721 (delta 26), reused 71 (delta 18), pack-reused 638 (from 2)\u001b[K\nReceiving objects: 100% (721/721), 146.48 MiB | 35.09 MiB/s, done.\nResolving deltas: 100% (286/286), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# standard library\nimport os\nimport sys\nimport math\nimport time\nimport shutil\nimport tarfile\nimport warnings\nfrom pathlib import Path\n\n# utility\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport time\n\n#PyTorch & torchvision\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms as T\nfrom torchvision.ops import box_iou\nimport torchvision.transforms as T\n\n#Albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\n#Custom repo modules \nrepo_path = \"/kaggle/working/PDLPR-algorithm/baseline_scr/detection\"\nsys.path.insert(0, repo_path)\nfrom model import LPDetectorFPN\nsys.path.remove(repo_path)\n\nrepo_path = \"/kaggle/working/PDLPR-algorithm/baseline_scr/recognition\"\nsys.path.insert(0, repo_path)\nfrom module import BaselineRecognizer\nsys.path.remove(repo_path)\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:02:52.161785Z","iopub.execute_input":"2025-07-21T14:02:52.162120Z","iopub.status.idle":"2025-07-21T14:03:01.909582Z","shell.execute_reply.started":"2025-07-21T14:02:52.162082Z","shell.execute_reply":"2025-07-21T14:03:01.908760Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## SETUP ENVIRONMENT","metadata":{}},{"cell_type":"code","source":"#downloading 50k imgs for train and 8k for test\n!gdown --folder https://drive.google.com/drive/folders/143HxhUrqkFIdfCzZQ3dA4Mqt8cjARCxx?usp=sharing -O datasets\n#https://drive.google.com/drive/u/1/folders/1Qirh0lsjdsroLHEmJDtS6sVXPQKalW6j","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:03:16.603624Z","iopub.execute_input":"2025-07-21T14:03:16.604289Z","iopub.status.idle":"2025-07-21T14:03:57.382823Z","shell.execute_reply.started":"2025-07-21T14:03:16.604263Z","shell.execute_reply":"2025-07-21T14:03:57.382058Z"}},"outputs":[{"name":"stdout","text":"Retrieving folder contents\nProcessing file 1rlOc7X2_C9vq2sm1ULBjNAgb_gy6CP8R ccpd_test.tar\nProcessing file 1hqZnTIOaRIaPPfN-juQKADCnE4ZJqqtO ccpd_train.tar\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1rlOc7X2_C9vq2sm1ULBjNAgb_gy6CP8R\nFrom (redirected): https://drive.google.com/uc?id=1rlOc7X2_C9vq2sm1ULBjNAgb_gy6CP8R&confirm=t&uuid=218ca55b-a186-4c42-85db-6e09889b2494\nTo: /kaggle/working/datasets/ccpd_test.tar\n100%|█████████████████████████████████████████| 557M/557M [00:02<00:00, 216MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1hqZnTIOaRIaPPfN-juQKADCnE4ZJqqtO\nFrom (redirected): https://drive.google.com/uc?id=1hqZnTIOaRIaPPfN-juQKADCnE4ZJqqtO&confirm=t&uuid=a376aeb5-d6a6-4fe0-838d-557225c88ea3\nTo: /kaggle/working/datasets/ccpd_train.tar\n100%|███████████████████████████████████████| 3.76G/3.76G [00:32<00:00, 115MB/s]\nDownload completed\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!gdown --fuzzy https://drive.google.com/file/d/1t0d9yFnCPztuQVm_l7CbXkx2NM9tCif6/view?usp=drive_link ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:04:01.262224Z","iopub.execute_input":"2025-07-21T14:04:01.262522Z","iopub.status.idle":"2025-07-21T14:04:05.117032Z","shell.execute_reply.started":"2025-07-21T14:04:01.262494Z","shell.execute_reply":"2025-07-21T14:04:05.116234Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1t0d9yFnCPztuQVm_l7CbXkx2NM9tCif6\nFrom (redirected): https://drive.google.com/uc?id=1t0d9yFnCPztuQVm_l7CbXkx2NM9tCif6&confirm=t&uuid=36cb27bf-dbb9-40f1-8b30-78863491ca98\nTo: /kaggle/working/base_rec_checkpoint_epoch50.pt\n100%|█████████████████████████████████████████| 264M/264M [00:01<00:00, 209MB/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# extracting the .tar archive.\ndef extract_tar_archive(archive_path, destination_path):\n\n    print(f\"Extracting the tar archive in:{archive_path}\")\n    with tarfile.open(archive_path, \"r\") as tar:\n        tar.extractall(path=destination_path)\n        \n    print(f\"Archive extracted in: {destination_path}\")\n\n#delete the .tar archive which now is useless.\ndef delete_tar_archive(path_tar_archive):\n    \n    if os.path.exists(path_tar_archive):\n        shutil.rmtree(path_tar_archive)\n        print(f\"Folder eliminated: {path_tar_archive}\")\n    else:\n        print(f\"Folder not found: {path_tar_archive}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:04:34.291713Z","iopub.execute_input":"2025-07-21T14:04:34.292619Z","iopub.status.idle":"2025-07-21T14:04:34.299212Z","shell.execute_reply.started":"2025-07-21T14:04:34.292573Z","shell.execute_reply":"2025-07-21T14:04:34.298267Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"archive_path_train = \"/kaggle/working/datasets/ccpd_train.tar\"\narchive_path_test = \"/kaggle/working/datasets/ccpd_test.tar\"\nextract_path = \"/kaggle/working/\"\n\n#when extracting the files, is important to eliminate the .tar archive which now occupy /kaggle/working space.\nextract_tar_archive(archive_path_train, extract_path)\nextract_tar_archive(archive_path_test, extract_path)\ndelete_tar_archive(\"/kaggle/working/datasets/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:04:36.448070Z","iopub.execute_input":"2025-07-21T14:04:36.448364Z","iopub.status.idle":"2025-07-21T14:04:52.910266Z","shell.execute_reply.started":"2025-07-21T14:04:36.448341Z","shell.execute_reply":"2025-07-21T14:04:52.909534Z"}},"outputs":[{"name":"stdout","text":"Extracting the tar archive in:/kaggle/working/datasets/ccpd_train.tar\nArchive extracted in: /kaggle/working/\nExtracting the tar archive in:/kaggle/working/datasets/ccpd_test.tar\nArchive extracted in: /kaggle/working/\nFolder eliminated: /kaggle/working/datasets/\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"PATH_WEIGHTS_DETECTION = \"/kaggle/working/PDLPR-algorithm/baseline_scr/detection/detec_weights.pt\"\nPATH_WEIGHTS_RECOGNITION = \"/kaggle/working/base_rec_checkpoint_epoch50.pt\"\nTEST_ROOT = \"/kaggle/working/ccpd_test\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:05:01.546488Z","iopub.execute_input":"2025-07-21T14:05:01.547181Z","iopub.status.idle":"2025-07-21T14:05:01.642817Z","shell.execute_reply.started":"2025-07-21T14:05:01.547155Z","shell.execute_reply":"2025-07-21T14:05:01.642097Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model_det = LPDetectorFPN()\nstate_dict = torch.load(PATH_WEIGHTS_DETECTION, map_location=\"cpu\") \nmodel_det.load_state_dict(state_dict)\n\nmodel_det.to(device).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:05:03.904375Z","iopub.execute_input":"2025-07-21T14:05:03.904642Z","iopub.status.idle":"2025-07-21T14:05:04.835710Z","shell.execute_reply.started":"2025-07-21T14:05:03.904620Z","shell.execute_reply":"2025-07-21T14:05:04.835059Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 202MB/s]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"LPDetectorFPN(\n  (stem): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (c3_lat): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (c4_lat): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (p3_conv): Sequential(\n    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (p4_conv): Sequential(\n    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (pool): AdaptiveAvgPool2d(output_size=1)\n  (regressor): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=256, out_features=256, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=256, out_features=4, bias=True)\n    (5): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"model_rec = BaselineRecognizer()\nstate_dict = torch.load(PATH_WEIGHTS_RECOGNITION, map_location=\"cpu\") \nmodel_rec.load_state_dict(state_dict[\"weights\"])\n\nmodel_rec.to(device).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:05:08.419209Z","iopub.execute_input":"2025-07-21T14:05:08.419706Z","iopub.status.idle":"2025-07-21T14:05:08.931933Z","shell.execute_reply.started":"2025-07-21T14:05:08.419682Z","shell.execute_reply":"2025-07-21T14:05:08.931238Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BaselineRecognizer(\n  (cnn): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (13): ReLU()\n  )\n  (rnn): LSTM(3072, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  (char_projection): Linear(in_features=1024, out_features=68, bias=True)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\",\n             \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n\nalphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N',\n             'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n\nads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R',\n       'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n\nunique_chars = set(provinces[:-1] + alphabets[:-1] + ads[:-1])  # escludi 'O'\nchar_list = sorted(list(unique_chars))  # ordinamento per coerenza\nchar_list = [\"-\"] + char_list\nchar2idx = {char: i for i, char in enumerate(char_list)}\nidx2char = {i: c for c, i in char2idx.items()}\n\nnum_classes = len(char_list)\nprint(\"Num classes: \", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:05:11.873051Z","iopub.execute_input":"2025-07-21T14:05:11.873550Z","iopub.status.idle":"2025-07-21T14:05:11.881783Z","shell.execute_reply.started":"2025-07-21T14:05:11.873525Z","shell.execute_reply":"2025-07-21T14:05:11.881053Z"}},"outputs":[{"name":"stdout","text":"Num classes:  68\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#extracting the metadata from each img in this format (image_path,x1_bbox,y1_bbox,x2_bbox,y2_bbox,plate_number)\ndef decode_plate(s):\n    \"this method is used for decoding the plate starting from the name of .jpg file\"\n    idx   = list(map(int, s.split(\"_\")))\n    try:\n        return provinces[idx[0]] + alphabets[idx[1]] + \"\".join(ads[i] for i in idx[2:])\n    except Exception:\n        return None\n\n#extracting the metadata from each img in this format (image_path,x1_bbox,y1_bbox,x2_bbox,y2_bbox)\ndef split_bbox(bbox_str):\n    \"extracting x1,y1,x2,y2, ex. '283___502_511___591'  →  ['283','502','511','591']\"\n    tokens = []\n    for seg in bbox_str.split(\"___\"):\n        tokens.extend(seg.split(\"_\"))\n    if len(tokens) == 4 and all(t.isdigit() for t in tokens):\n        return map(int, tokens)\n    return (None,)*4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:05:13.921361Z","iopub.execute_input":"2025-07-21T14:05:13.921952Z","iopub.status.idle":"2025-07-21T14:05:13.927033Z","shell.execute_reply.started":"2025-07-21T14:05:13.921925Z","shell.execute_reply":"2025-07-21T14:05:13.926264Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## TEST PHASE","metadata":{}},{"cell_type":"code","source":"folder = \"/kaggle/working/ccpd_test\"\nrows = []\n\nfor root, _, files in os.walk(folder):\n    for fname in files:\n        if not fname.endswith(\".jpg\"):\n            continue\n\n        parts = fname[:-4].split(\"-\")\n        if len(parts) < 6:\n            continue\n\n        x1, y1, x2, y2 = split_bbox(parts[2])\n        plate = decode_plate(parts[4])\n        full_path = os.path.join(root, fname)\n\n        rows.append({\n            \"image_path\": full_path,\n            \"x1_bbox\": x1,\n            \"y1_bbox\": y1,\n            \"x2_bbox\": x2,\n            \"y2_bbox\": y2,\n            \"plate_number\": plate\n        })\n\ndf = pd.DataFrame(rows)\ndf[\"subset\"] = df[\"image_path\"].apply(lambda p: Path(p).parts[-2])\nprint(f\"Dataset created: {len(df)} rows\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:05:16.381744Z","iopub.execute_input":"2025-07-21T14:05:16.382461Z","iopub.status.idle":"2025-07-21T14:05:16.517762Z","shell.execute_reply.started":"2025-07-21T14:05:16.382434Z","shell.execute_reply":"2025-07-21T14:05:16.517068Z"}},"outputs":[{"name":"stdout","text":"Dataset created: 8000 rows\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                          image_path  x1_bbox  y1_bbox  \\\n0  /kaggle/working/ccpd_test/fn/0777-6_20-113___4...      113      469   \n1  /kaggle/working/ccpd_test/fn/2067-3_2-0___321_...        0      321   \n2  /kaggle/working/ccpd_test/fn/0068-3_4-198___47...      198      471   \n3  /kaggle/working/ccpd_test/fn/0836-4_3-111___44...      111      446   \n4  /kaggle/working/ccpd_test/fn/0675-18_28-194___...      194      356   \n\n   x2_bbox  y2_bbox plate_number subset  \n0      529      625      皖AY165D     fn  \n1      688      572      皖AS276E     fn  \n2      299      528      皖AX868C     fn  \n3      553      604      皖A0X569     fn  \n4      482      552      皖AT0581     fn  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>x1_bbox</th>\n      <th>y1_bbox</th>\n      <th>x2_bbox</th>\n      <th>y2_bbox</th>\n      <th>plate_number</th>\n      <th>subset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/ccpd_test/fn/0777-6_20-113___4...</td>\n      <td>113</td>\n      <td>469</td>\n      <td>529</td>\n      <td>625</td>\n      <td>皖AY165D</td>\n      <td>fn</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/ccpd_test/fn/2067-3_2-0___321_...</td>\n      <td>0</td>\n      <td>321</td>\n      <td>688</td>\n      <td>572</td>\n      <td>皖AS276E</td>\n      <td>fn</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/ccpd_test/fn/0068-3_4-198___47...</td>\n      <td>198</td>\n      <td>471</td>\n      <td>299</td>\n      <td>528</td>\n      <td>皖AX868C</td>\n      <td>fn</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/ccpd_test/fn/0836-4_3-111___44...</td>\n      <td>111</td>\n      <td>446</td>\n      <td>553</td>\n      <td>604</td>\n      <td>皖A0X569</td>\n      <td>fn</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/ccpd_test/fn/0675-18_28-194___...</td>\n      <td>194</td>\n      <td>356</td>\n      <td>482</td>\n      <td>552</td>\n      <td>皖AT0581</td>\n      <td>fn</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# CONFIG\nIOU_THR      = 0.60          # detection is accepted if IoU ≥ 0.60\nDET_IN       = 224           # detector input side\nREC_H, REC_W = 48, 144       # recogniser input (HxW)\nBATCH_SIZE   = 5             # dataloader batch for detector\n\n\ntf_det = T.Compose([\n    T.ToPILImage(),\n    T.Resize((DET_IN, DET_IN), interpolation=T.InterpolationMode.BILINEAR),\n    T.ToTensor()\n])\n\ndef collate_fn(batch):\n    det_batch   = torch.stack([b[0] for b in batch])   # B×3×224×224\n    rgb_list    = [b[1] for b in batch]\n    bbox_list   = [b[2] for b in batch]                # list of tensors\n    plate_list  = [b[3] for b in batch]\n    hw_list     = [b[4] for b in batch]\n    return det_batch, rgb_list, bbox_list, plate_list, hw_list\n\nloader = DataLoader(CCPDTestDataset(df),\n                    batch_size=BATCH_SIZE,\n                    shuffle=False, num_workers=4,\n                    collate_fn=collate_fn, pin_memory=True)\n\n\n# HELPERS\nidx2char = {i: c for c, i in char2idx.items()}\n\ndef logits_to_plate(logits: torch.Tensor) -> str:\n    return ''.join(idx2char[i] for i in logits.argmax(-1).tolist())\n\ndef xyxy_iou(a, b):                      # tensors length‑4\n    ix = (min(a[2], b[2]) - max(a[0], b[0])).clamp_(0)\n    iy = (min(a[3], b[3]) - max(a[1], b[1])).clamp_(0)\n    inter = ix * iy\n    area_a = (a[2]-a[0])*(a[3]-a[1])\n    area_b = (b[2]-b[0])*(b[3]-b[1])\n    return inter / (area_a + area_b - inter + 1e-7)\n\ntf_rec = T.Compose([\n    T.ToPILImage(),\n    T.Resize((REC_H, REC_W), interpolation=T.InterpolationMode.BILINEAR),\n    T.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:23:50.340244Z","iopub.execute_input":"2025-07-21T14:23:50.341107Z","iopub.status.idle":"2025-07-21T14:23:50.353711Z","shell.execute_reply.started":"2025-07-21T14:23:50.341068Z","shell.execute_reply":"2025-07-21T14:23:50.352950Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class CCPDTestDataset(torch.utils.data.Dataset):\n    \"\"\"Returns:\n       det_tensor : 3×224×224   (for detector)\n       rgb_orig   : H×W×3  (np.uint8) original image for cropping\n       bbox_gt    : tensor[4]   (x1,y1,x2,y2) ground‑truth\n       plate_str  : str         ground‑truth plate\n       orig_hw    : (H,W)       original size\n    \"\"\"\n    def __init__(self, dataframe):\n        self.df = dataframe.reset_index(drop=True)\n\n    def __len__(self):  return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        rgb = cv2.cvtColor(cv2.imread(row.image_path), cv2.COLOR_BGR2RGB)\n        H, W = rgb.shape[:2]\n\n        det_tensor = tf_det(rgb)          # torch tensor 3×224×224\n        bbox_gt = torch.tensor([row.x1_bbox, row.y1_bbox,\n                                row.x2_bbox, row.y2_bbox], dtype=torch.float32)\n        return det_tensor, rgb, bbox_gt, row.plate_number, (H, W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:23:54.983948Z","iopub.execute_input":"2025-07-21T14:23:54.984511Z","iopub.status.idle":"2025-07-21T14:23:54.989744Z","shell.execute_reply.started":"2025-07-21T14:23:54.984487Z","shell.execute_reply":"2025-07-21T14:23:54.989061Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df[\"subset\"] = df.image_path.apply(lambda p: Path(p).parent.name)\nresults = []\n\nfor subset in sorted(df['subset'].unique()):\n    print(f\"\\nEvaluating subset: {subset}\")\n    df_subset = df[df.subset == subset].copy()\n\n    # DataLoader\n    loader = DataLoader(CCPDTestDataset(df_subset),\n                        batch_size=BATCH_SIZE,\n                        shuffle=False,\n                        num_workers=4,\n                        collate_fn=collate_fn,\n                        pin_memory=True)\n\n    total_imgs = correct_plates = 0\n    t_start = time.perf_counter()\n\n    for det_batch, rgb_list, bbox_list, plate_gt_list, hw_list in loader:\n        B = det_batch.size(0)\n\n        # Detector forward\n        with torch.no_grad():\n            preds = model_det(det_batch.to(device)).cpu()  # Bx4\n\n        # Iterate over batch\n        for i in range(B):\n            total_imgs += 1\n            H0, W0 = hw_list[i]\n            cx, cy, w, h = (preds[i] * DET_IN).tolist()\n\n            # Back-project to original image size\n            x1 = (cx - w/2) * W0 / DET_IN\n            y1 = (cy - h/2) * H0 / DET_IN\n            x2 = (cx + w/2) * W0 / DET_IN\n            y2 = (cy + h/2) * H0 / DET_IN\n            pred_xy = torch.tensor([x1, y1, x2, y2])\n\n            iou = xyxy_iou(pred_xy, bbox_list[i])\n            if iou < IOU_THR:\n                continue  # detection failed\n\n            # Crop & recognise\n            x1i, y1i, x2i, y2i = map(int, [max(0,x1), max(0,y1), min(W0,x2), min(H0,y2)])\n            if x2i <= x1i or y2i <= y1i:\n                continue\n\n            crop_rgb = rgb_list[i][y1i:y2i, x1i:x2i]\n            rec_in = tf_rec(crop_rgb).unsqueeze(0).to(device)\n            with torch.no_grad():\n                logits = model_rec(rec_in)[0].cpu()\n\n            if logits_to_plate(logits) == plate_gt_list[i]:\n                correct_plates += 1\n\n    t_end = time.perf_counter()\n    fps = total_imgs / (t_end - t_start)\n    acc = correct_plates / total_imgs if total_imgs else 0.0\n\n    print(f\"Subset: {subset:12} | Accuracy: {acc:.3f} | FPS: {fps:.1f}\")\n    results.append((subset, acc, fps))\n\n# Summary table\nprint(\"\\n\" + \"-\"*40)\nprint(f\"{'Subset':12} | {'Acc':>7} | {'FPS':>6}\")\nprint(\"-\"*40)\nfor s, a, f in results:\n    print(f\"{s:12} | {a:7.3f} | {f:6.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:23:57.553874Z","iopub.execute_input":"2025-07-21T14:23:57.554170Z","iopub.status.idle":"2025-07-21T14:25:30.850744Z","shell.execute_reply.started":"2025-07-21T14:23:57.554146Z","shell.execute_reply":"2025-07-21T14:25:30.849838Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating subset: base\nSubset: base         | Accuracy: 0.997 | FPS: 87.7\n\nEvaluating subset: blur\nSubset: blur         | Accuracy: 0.777 | FPS: 88.5\n\nEvaluating subset: challenge\nSubset: challenge    | Accuracy: 0.826 | FPS: 90.3\n\nEvaluating subset: db\nSubset: db           | Accuracy: 0.764 | FPS: 90.3\n\nEvaluating subset: fn\nSubset: fn           | Accuracy: 0.800 | FPS: 84.6\n\nEvaluating subset: rotate\nSubset: rotate       | Accuracy: 0.933 | FPS: 80.7\n\nEvaluating subset: tilt\nSubset: tilt         | Accuracy: 0.865 | FPS: 81.3\n\nEvaluating subset: weather\nSubset: weather      | Accuracy: 0.987 | FPS: 84.3\n\n----------------------------------------\nSubset       |     Acc |    FPS\n----------------------------------------\nbase         |   0.997 |   87.7\nblur         |   0.777 |   88.5\nchallenge    |   0.826 |   90.3\ndb           |   0.764 |   90.3\nfn           |   0.800 |   84.6\nrotate       |   0.933 |   80.7\ntilt         |   0.865 |   81.3\nweather      |   0.987 |   84.3\n","output_type":"stream"}],"execution_count":17}]}