{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!rm -rf /kaggle/working/PDLPR-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/giankev/PDLPR-algorithm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:37:10.407862Z",
     "iopub.status.busy": "2025-07-20T23:37:10.407564Z",
     "iopub.status.idle": "2025-07-20T23:37:14.662057Z",
     "shell.execute_reply": "2025-07-20T23:37:14.661461Z",
     "shell.execute_reply.started": "2025-07-20T23:37:10.407839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# utility\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "#PyTorch & torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms as T\n",
    "from torchvision.ops import box_iou\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#Albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "#Custom repo modules \n",
    "repo_path = \"/kaggle/working/PDLPR-algorithm/baseline_scr/detection\"\n",
    "sys.path.insert(0, repo_path)\n",
    "from model import LPDetectorFPN\n",
    "from trainer import set_seed, train, train_one_epoch, evaluate, ciou_loss, cxcywh_to_xyxy\n",
    "sys.path.remove(repo_path)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#downloading 50k imgs for train and 8k for test\n",
    "!gdown --folder https://drive.google.com/drive/folders/143HxhUrqkFIdfCzZQ3dA4Mqt8cjARCxx?usp=sharing -O datasets\n",
    "#https://drive.google.com/drive/u/1/folders/1Qirh0lsjdsroLHEmJDtS6sVXPQKalW6j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extracting the .tar archive.\n",
    "def extract_tar_archive(archive_path, destination_path):\n",
    "\n",
    "    print(f\"Extracting the tar archive in:{archive_path}\")\n",
    "    with tarfile.open(archive_path, \"r\") as tar:\n",
    "        tar.extractall(path=destination_path)\n",
    "        \n",
    "    print(f\"Archive extracted in: {destination_path}\")\n",
    "\n",
    "#delete the .tar archive which now is useless.\n",
    "def delete_tar_archive(path_tar_archive):\n",
    "    \n",
    "    if os.path.exists(path_tar_archive):\n",
    "        shutil.rmtree(path_tar_archive)\n",
    "        print(f\"Folder eliminated: {path_tar_archive}\")\n",
    "    else:\n",
    "        print(f\"Folder not found: {path_tar_archive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "archive_path_train = \"/kaggle/working/datasets/ccpd_train.tar\"\n",
    "archive_path_test = \"/kaggle/working/datasets/ccpd_test.tar\"\n",
    "extract_path = \"/kaggle/working/\"\n",
    "\n",
    "#when extracting the files, is important to eliminate the .tar archive which now occupy /kaggle/working space.\n",
    "extract_tar_archive(archive_path_train, extract_path)\n",
    "extract_tar_archive(archive_path_test, extract_path)\n",
    "delete_tar_archive(\"/kaggle/working/datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:37:22.243606Z",
     "iopub.status.busy": "2025-07-20T23:37:22.242901Z",
     "iopub.status.idle": "2025-07-20T23:37:22.249085Z",
     "shell.execute_reply": "2025-07-20T23:37:22.248324Z",
     "shell.execute_reply.started": "2025-07-20T23:37:22.243579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#extracting the metadata from each img in this format (image_path,x1_bbox,y1_bbox,x2_bbox,y2_bbox)\n",
    "def split_bbox(bbox_str):\n",
    "    \"extracting x1,y1,x2,y2, ex. '283___502_511___591'  â†’  ['283','502','511','591']\"\n",
    "    tokens = []\n",
    "    for seg in bbox_str.split(\"___\"):\n",
    "        tokens.extend(seg.split(\"_\"))\n",
    "    if len(tokens) == 4 and all(t.isdigit() for t in tokens):\n",
    "        return map(int, tokens)\n",
    "    return (None,)*4\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}\")\n",
    "    print(f\"Trainable parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:37:24.596409Z",
     "iopub.status.busy": "2025-07-20T23:37:24.595930Z",
     "iopub.status.idle": "2025-07-20T23:37:24.604340Z",
     "shell.execute_reply": "2025-07-20T23:37:24.603691Z",
     "shell.execute_reply.started": "2025-07-20T23:37:24.596384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_resize_bbox(df: pd.DataFrame,\n",
    "                           out_dir: str = \"preproc224\",\n",
    "                           img_size: int = 224,\n",
    "                           save_as_pt: bool = False,\n",
    "                           quality: int = 95) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resize images and bounding boxes, saving the output and returning an updated DataFrame.\n",
    "\n",
    "     df         : DataFrame containing columns: image_path, x1_bbox, y1_bbox, x2_bbox, y2_bbox\n",
    "     out_dir    : output folder to save resized images or tensors\n",
    "     img_size   : target size (square, e.g. 224)\n",
    "     save_as_pt : if True, saves images as .pt tensors; else as JPEG\n",
    "     quality    : JPEG quality (only used if save_as_pt is False)\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    records = []\n",
    "    to_tensor = torch.from_numpy  # shortcut for converting numpy to tensor\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preâ€‘resize\"):\n",
    "        # Load image and convert to RGB\n",
    "        img = Image.open(row.image_path).convert(\"RGB\")\n",
    "        w0, h0 = img.size  # original dimensions\n",
    "\n",
    "        # Resize image to target size (e.g., 224x224) using bilinear interpolation\n",
    "        img = img.resize((img_size, img_size), Image.BILINEAR)\n",
    "\n",
    "        # Scale bounding box coordinates to the new image size\n",
    "        sx, sy = img_size / w0, img_size / h0\n",
    "        x1 = row.x1_bbox * sx\n",
    "        y1 = row.y1_bbox * sy\n",
    "        x2 = row.x2_bbox * sx\n",
    "        y2 = row.y2_bbox * sy\n",
    "\n",
    "        # Get filename without extension\n",
    "        stem = Path(row.image_path).stem\n",
    "\n",
    "        if save_as_pt:\n",
    "            # Convert image to tensor and normalize to [0,1]\n",
    "            tensor = to_tensor(np.array(img)).permute(2, 0, 1).float() / 255\n",
    "            path_out = out_dir / f\"{stem}.pt\"\n",
    "            torch.save(tensor, path_out)\n",
    "        else:\n",
    "            # Save image as JPEG with given quality\n",
    "            path_out = out_dir / f\"{stem}.jpg\"\n",
    "            img.save(path_out, format=\"JPEG\", quality=quality, optimize=True)\n",
    "\n",
    "        # Store updated info (new image path and scaled bbox) in the new DataFrame\n",
    "        records.append({\n",
    "            \"image_path\": str(path_out),\n",
    "            \"x1_bbox\": x1, \"y1_bbox\": y1,\n",
    "            \"x2_bbox\": x2, \"y2_bbox\": y2,\n",
    "            **{c: row[c] for c in df.columns if c not in (\n",
    "               \"image_path\", \"x1_bbox\", \"y1_bbox\", \"x2_bbox\", \"y2_bbox\")}\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:37:30.502309Z",
     "iopub.status.busy": "2025-07-20T23:37:30.502021Z",
     "iopub.status.idle": "2025-07-20T23:37:30.801682Z",
     "shell.execute_reply": "2025-07-20T23:37:30.800874Z",
     "shell.execute_reply.started": "2025-07-20T23:37:30.502287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows number: 50000\n",
      "Columns numner: 5\n",
      "Shape: (50000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>x1_bbox</th>\n",
       "      <th>y1_bbox</th>\n",
       "      <th>x2_bbox</th>\n",
       "      <th>y2_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/working/ccpd_subset_base/train/0254310...</td>\n",
       "      <td>182</td>\n",
       "      <td>424</td>\n",
       "      <td>500</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/working/ccpd_subset_base/train/0188362...</td>\n",
       "      <td>220</td>\n",
       "      <td>544</td>\n",
       "      <td>456</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/working/ccpd_subset_base/train/0263146...</td>\n",
       "      <td>211</td>\n",
       "      <td>483</td>\n",
       "      <td>460</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/working/ccpd_subset_base/train/0320761...</td>\n",
       "      <td>187</td>\n",
       "      <td>520</td>\n",
       "      <td>478</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/working/ccpd_subset_base/train/0393247...</td>\n",
       "      <td>147</td>\n",
       "      <td>463</td>\n",
       "      <td>509</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  x1_bbox  y1_bbox  \\\n",
       "0  /kaggle/working/ccpd_subset_base/train/0254310...      182      424   \n",
       "1  /kaggle/working/ccpd_subset_base/train/0188362...      220      544   \n",
       "2  /kaggle/working/ccpd_subset_base/train/0263146...      211      483   \n",
       "3  /kaggle/working/ccpd_subset_base/train/0320761...      187      520   \n",
       "4  /kaggle/working/ccpd_subset_base/train/0393247...      147      463   \n",
       "\n",
       "   x2_bbox  y2_bbox  \n",
       "0      500      530  \n",
       "1      456      637  \n",
       "2      460      600  \n",
       "3      478      639  \n",
       "4      509      573  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"/kaggle/working/ccpd_subset_base/train\"\n",
    "rows   = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    if not fname.endswith(\".jpg\"): continue\n",
    "\n",
    "    parts = fname[:-4].split(\"-\")           \n",
    "    if len(parts) < 6:\n",
    "        continue #the ccpd file name is wrong           \n",
    "\n",
    "    x1,y1,x2,y2 = split_bbox(parts[2])          \n",
    "    \n",
    "    rows.append({\n",
    "        \"image_path\": os.path.join(folder, fname),\n",
    "        \"x1_bbox\": x1, \"y1_bbox\": y1,\n",
    "        \"x2_bbox\": x2, \"y2_bbox\": y2\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Rows number:\", len(df))         \n",
    "print(\"Columns numner:\", df.shape[1])\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PlateDatasetFastAug(Dataset):\n",
    "    def __init__(self, df, augment=True, img_size=224):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.augment = augment\n",
    "        self.img_size = img_size\n",
    "\n",
    "        if augment:\n",
    "            self.pipeline = A.Compose([\n",
    "                # Blurring\n",
    "                A.OneOf([\n",
    "                    A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "                    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "                ], p=0.6),\n",
    "                \n",
    "                # Brightness & contrast variation\n",
    "                A.OneOf([\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.6, -0.3), contrast_limit=0.2, p=0.6),\n",
    "                ], p=0.5),\n",
    "                \n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "                A.ColorJitter(0.2, 0.2, 0.2, 0.1, p=0.4),\n",
    "                \n",
    "                # Random crop or zoom-in\n",
    "                A.OneOf([\n",
    "                    A.Compose([\n",
    "                        A.LongestMaxSize(max_size=int(img_size * 0.6)),\n",
    "                        A.PadIfNeeded(img_size, img_size,\n",
    "                                      border_mode=cv2.BORDER_REPLICATE, value=(0, 0, 0)),\n",
    "                    ], p=0.5),\n",
    "                    A.RandomResizedCrop(size=(img_size, img_size), scale=(0.8, 1), ratio=(1.0, 1.0)),\n",
    "                ], p=0.35),\n",
    "\n",
    "                A.Perspective(scale=(0.03, 0.06), p=0.4),\n",
    "\n",
    "                # CLAHE or shadow simulation\n",
    "                A.OneOf([\n",
    "                    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "                    A.RandomShadow(num_shadows_lower=1, num_shadows_upper=2,\n",
    "                                   shadow_dimension=4, shadow_roi=(0, 0.4, 1, 1), p=0.5),\n",
    "                ], p=0.3),\n",
    "\n",
    "                # Downscale â†’ simulates low resolution\n",
    "                A.OneOf([\n",
    "                    A.Downscale(scale_min=0.3, scale_max=0.5, interpolation=cv2.INTER_LINEAR, p=1.0)\n",
    "                ], p=0.25),\n",
    "\n",
    "                ToTensorV2()\n",
    "            ], bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"labels\"]))\n",
    "        \n",
    "        else:\n",
    "            self.pipeline = A.Compose([\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load and convert image to RGB\n",
    "        img = cv2.cvtColor(cv2.imread(row.image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize bounding box to [0, 1]\n",
    "        x1, y1, x2, y2 = row.x1_bbox, row.y1_bbox, row.x2_bbox, row.y2_bbox\n",
    "        cx = (x1 + x2) / 2 / self.img_size\n",
    "        cy = (y1 + y2) / 2 / self.img_size\n",
    "        w  = (x2 - x1) / self.img_size\n",
    "        h  = (y2 - y1) / self.img_size\n",
    "        bbox_yolo = [cx, cy, w, h]\n",
    "\n",
    "        # Apply augmentation (or just transform)\n",
    "        transformed = self.pipeline(\n",
    "            image=img,\n",
    "            bboxes=[bbox_yolo],\n",
    "            labels=[0]\n",
    "        )\n",
    "\n",
    "        img_tensor = transformed[\"image\"].float() / 255.0\n",
    "        bbox = torch.tensor(transformed[\"bboxes\"][0], dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# split the dataset for training phase.\n",
    "df_train, df_val = train_test_split(df, test_size=0.02, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Train set: {len(df_train)} img\")\n",
    "print(f\"Val set:   {len(df_val)} img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Prepocess imgs into 224x224\n",
    "df_train_224 = preprocess_resize_bbox(df_train, \"train224\", img_size=224, save_as_pt=False)\n",
    "df_val_224   = preprocess_resize_bbox(df_val,   \"val224\",   img_size=224, save_as_pt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = PlateDatasetFastAug(df_train_224, augment=True)\n",
    "val_ds   = PlateDatasetFastAug(df_val_224, augment=False)\n",
    "\n",
    "dl_train = DataLoader(train_ds, batch_size=128, shuffle=True,\n",
    "                      num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "dl_val   = DataLoader(val_ds, batch_size=128, shuffle=False,\n",
    "                      num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LPDetectorFPN()      \n",
    "count_parameters(model)\n",
    "model  = train(model, dl_train, dl_val,\n",
    "               epochs=20, lr=1e-4, device=device)\n",
    "\n",
    "torch.save(model.state_dict(), \"/kaggle/working/lp2_detectorr.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:38:08.532678Z",
     "iopub.status.busy": "2025-07-20T23:38:08.532359Z",
     "iopub.status.idle": "2025-07-20T23:38:08.638566Z",
     "shell.execute_reply": "2025-07-20T23:38:08.637786Z",
     "shell.execute_reply.started": "2025-07-20T23:38:08.532623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8000\n",
      "Cols: 6\n",
      "Shape: (8000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>image_path</th>\n",
       "      <th>x1_bbox</th>\n",
       "      <th>y1_bbox</th>\n",
       "      <th>x2_bbox</th>\n",
       "      <th>y2_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weather</td>\n",
       "      <td>/kaggle/working/ccpd_test/weather/0304-9_19-23...</td>\n",
       "      <td>230</td>\n",
       "      <td>484</td>\n",
       "      <td>457</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weather</td>\n",
       "      <td>/kaggle/working/ccpd_test/weather/0295-4_1-253...</td>\n",
       "      <td>253</td>\n",
       "      <td>428</td>\n",
       "      <td>490</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weather</td>\n",
       "      <td>/kaggle/working/ccpd_test/weather/0322-12_12-3...</td>\n",
       "      <td>304</td>\n",
       "      <td>384</td>\n",
       "      <td>502</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather</td>\n",
       "      <td>/kaggle/working/ccpd_test/weather/0455-18_17-4...</td>\n",
       "      <td>438</td>\n",
       "      <td>508</td>\n",
       "      <td>659</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weather</td>\n",
       "      <td>/kaggle/working/ccpd_test/weather/0140-0_0-134...</td>\n",
       "      <td>134</td>\n",
       "      <td>387</td>\n",
       "      <td>326</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subset                                         image_path  x1_bbox  \\\n",
       "0  weather  /kaggle/working/ccpd_test/weather/0304-9_19-23...      230   \n",
       "1  weather  /kaggle/working/ccpd_test/weather/0295-4_1-253...      253   \n",
       "2  weather  /kaggle/working/ccpd_test/weather/0322-12_12-3...      304   \n",
       "3  weather  /kaggle/working/ccpd_test/weather/0455-18_17-4...      438   \n",
       "4  weather  /kaggle/working/ccpd_test/weather/0140-0_0-134...      134   \n",
       "\n",
       "   y1_bbox  x2_bbox  y2_bbox  \n",
       "0      484      457      596  \n",
       "1      428      490      532  \n",
       "2      384      502      520  \n",
       "3      508      659      680  \n",
       "4      387      326      448  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root   = Path(\"/kaggle/working/ccpd_test\")   \n",
    "rows   = []\n",
    "\n",
    "for jpg in root.rglob(\"*.jpg\"):\n",
    "    fname = jpg.name\n",
    "    parts = fname[:-4].split(\"-\")            \n",
    "    if len(parts) < 6:\n",
    "        continue  #wrong name file                         \n",
    "\n",
    "    try:\n",
    "        x1, y1, x2, y2 = split_bbox(parts[2])\n",
    "    except Exception as e:\n",
    "        print(\"skip\", jpg, e)\n",
    "        continue\n",
    "\n",
    "    rows.append({\n",
    "        \"subset\": jpg.parent.name,    \n",
    "        \"image_path\": str(jpg),\n",
    "        \"x1_bbox\": x1, \"y1_bbox\": y1,\n",
    "        \"x2_bbox\": x2, \"y2_bbox\": y2,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Rows:\",   len(df))\n",
    "print(\"Cols:\",   df.shape[1])\n",
    "print(\"Shape:\",  df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:38:13.989552Z",
     "iopub.status.busy": "2025-07-20T23:38:13.988820Z",
     "iopub.status.idle": "2025-07-20T23:40:10.320825Z",
     "shell.execute_reply": "2025-07-20T23:40:10.319976Z",
     "shell.execute_reply.started": "2025-07-20T23:38:13.989517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "base      :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_base      | Acc@0.7: 0.9820 | IoU: 0.863 | L1: 0.0001 | img: 1000 | FPS: 87.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "blur      :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_blur      | Acc@0.7: 0.8310 | IoU: 0.794 | L1: 0.0001 | img: 1000 | FPS: 94.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "challenge :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_challenge | Acc@0.7: 0.8630 | IoU: 0.798 | L1: 0.0001 | img: 1000 | FPS: 96.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "db        :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_db        | Acc@0.7: 0.7470 | IoU: 0.765 | L1: 0.0003 | img: 1000 | FPS: 93.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fn        :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_fn        | Acc@0.7: 0.7400 | IoU: 0.758 | L1: 0.0006 | img: 1000 | FPS: 94.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rotate    :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_rotate    | Acc@0.7: 0.9340 | IoU: 0.810 | L1: 0.0002 | img: 1000 | FPS: 96.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tilt      :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_tilt      | Acc@0.7: 0.8170 | IoU: 0.780 | L1: 0.0003 | img: 1000 | FPS: 99.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "weather   :   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCPD_weather   | Acc@0.7: 0.9850 | IoU: 0.867 | L1: 0.0001 | img: 1000 | FPS: 96.0\n",
      "\n",
      "ðŸ”¹ GLOBAL | Acc@0.7: 0.8624 | IoU: 0.804 | L1: 0.0002 | FPS: 94.7\n"
     ]
    }
   ],
   "source": [
    "# parameters setup\n",
    "IOU_THR  = 0.7            \n",
    "IMG_SIZE = 224            \n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#model init\n",
    "#otherwise you can use the weights from the previous training phase \n",
    "weights_github = \"/kaggle/working/PDLPR-algorithm/baseline_scr/detection/detec_weights.pt\" \n",
    "detector = LPDetectorFPN().to(device)\n",
    "detector.load_state_dict(torch.load(weights_github, map_location=device))\n",
    "detector.eval()\n",
    "\n",
    "#utils\n",
    "tfm = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BILINEAR),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def bbox_iou(pred, tgt, eps=1e-7):\n",
    "    px1, py1 = pred[:, 0] - pred[:, 2] / 2, pred[:, 1] - pred[:, 3] / 2\n",
    "    px2, py2 = pred[:, 0] + pred[:, 2] / 2, pred[:, 1] + pred[:, 3] / 2\n",
    "    tx1, ty1 = tgt[:, 0] - tgt[:, 2] / 2, tgt[:, 1] - tgt[:, 3] / 2\n",
    "    tx2, ty2 = tgt[:, 0] + tgt[:, 2] / 2, tgt[:, 1] + tgt[:, 3] / 2\n",
    "\n",
    "    inter_w = (torch.min(px2, tx2) - torch.max(px1, tx1)).clamp(min=0)\n",
    "    inter_h = (torch.min(py2, ty2) - torch.max(py1, ty1)).clamp(min=0)\n",
    "    inter   = inter_w * inter_h\n",
    "    area_p  = (px2 - px1) * (py2 - py1)\n",
    "    area_t  = (tx2 - tx1) * (ty2 - ty1)\n",
    "    union   = area_p + area_t - inter + eps\n",
    "    return inter / union\n",
    "\n",
    "loss_fn = torch.nn.SmoothL1Loss(reduction=\"mean\")\n",
    "\n",
    "#evaluation subset\n",
    "@torch.no_grad()\n",
    "def eval_subset(df_sub, name):\n",
    "    iou_list, loss_list = [], []\n",
    "    correct, t_forward, n_imgs = 0, 0.0, 0\n",
    "\n",
    "    for _, row in tqdm(df_sub.iterrows(), total=len(df_sub),\n",
    "                       desc=f\"{name:10}\", leave=False):\n",
    "\n",
    "        img_bgr = cv2.imread(row.image_path);  n_imgs += 1\n",
    "        if img_bgr is None: continue\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        h0, w0  = img_rgb.shape[:2]\n",
    "\n",
    "        # preprocessing + forward (timed)\n",
    "        t0 = time.time()\n",
    "        img_t = tfm(img_rgb).unsqueeze(0).to(device)\n",
    "        pred  = detector(img_t)[0].cpu()              # cxcywh âˆˆ [0,1]\n",
    "        t_forward += (time.time() - t0)\n",
    "\n",
    "        # ground truth scale to IMGSIZE\n",
    "        sx, sy = IMG_SIZE / w0, IMG_SIZE / h0\n",
    "        x1, y1, x2, y2 = row.x1_bbox, row.y1_bbox, row.x2_bbox, row.y2_bbox\n",
    "        x1_r, y1_r, x2_r, y2_r = x1*sx, y1*sy, x2*sx, y2*sy\n",
    "        tgt = torch.tensor([(x1_r + x2_r) / (2*IMG_SIZE),\n",
    "                            (y1_r + y2_r) / (2*IMG_SIZE),\n",
    "                            (x2_r - x1_r) / IMG_SIZE,\n",
    "                            (y2_r - y1_r) / IMG_SIZE])\n",
    "\n",
    "        #metrics\n",
    "        iou  = bbox_iou(pred.unsqueeze(0), tgt.unsqueeze(0)).item()\n",
    "        loss = loss_fn(pred, tgt).item()\n",
    "        iou_list.append(iou);  loss_list.append(loss)\n",
    "        if iou >= IOU_THR: correct += 1\n",
    "\n",
    "    acc   = correct / n_imgs\n",
    "    m_iou = np.mean(iou_list)\n",
    "    m_l1  = np.mean(loss_list)\n",
    "    fps   = n_imgs / t_forward if t_forward else 0.0\n",
    "\n",
    "    print(f\"CCPD_{name:<9} | Acc@{IOU_THR}: {acc:.4f} | IoU: {m_iou:.3f} \"\n",
    "          f\"| L1: {m_l1:.4f} | img: {n_imgs} | FPS: {fps:.1f}\")\n",
    "    return n_imgs, acc, m_iou, m_l1, t_forward\n",
    "\n",
    "#subset loop\n",
    "df[\"subset\"] = df.image_path.apply(lambda p: Path(p).parts[-2])\n",
    "\n",
    "g_imgs = g_acc = g_iou = g_l1 = total_time = 0.0\n",
    "for sub in sorted(df.subset.unique()):\n",
    "    n, acc, miou, mloss, t = eval_subset(df[df.subset == sub], sub)\n",
    "    g_imgs += n;  g_acc += acc*n;  g_iou += miou*n;  g_l1 += mloss*n;  total_time += t\n",
    "\n",
    "if g_imgs:\n",
    "    print(f\"\\nðŸ”¹ GLOBAL | Acc@{IOU_THR}: {g_acc/g_imgs:.4f} \"\n",
    "          f\"| IoU: {g_iou/g_imgs:.3f} \"\n",
    "          f\"| L1: {g_l1/g_imgs:.4f} \"\n",
    "          f\"| FPS: {g_imgs/total_time:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
