{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:57:15.348307Z",
     "iopub.status.busy": "2025-07-08T09:57:15.347798Z",
     "iopub.status.idle": "2025-07-08T09:57:15.351686Z",
     "shell.execute_reply": "2025-07-08T09:57:15.350940Z",
     "shell.execute_reply.started": "2025-07-08T09:57:15.348285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "working_on_kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:57:17.449668Z",
     "iopub.status.busy": "2025-07-08T09:57:17.449415Z",
     "iopub.status.idle": "2025-07-08T09:57:42.605754Z",
     "shell.execute_reply": "2025-07-08T09:57:42.605028Z",
     "shell.execute_reply.started": "2025-07-08T09:57:17.449649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if working_on_kaggle:\n",
    "    !pip install --quiet gdown\n",
    "    !apt-get install -y fonts-noto-cjk > /dev/null\n",
    "\n",
    "    import os\n",
    "   \n",
    "    from getpass import getpass\n",
    "    \n",
    "    token = getpass('Your GitHub token: ')\n",
    "    username = \"iamlucaconti\"\n",
    "    repo_name = \"PDDLR-algorithm\"\n",
    "    git_url = f\"https://{username}:{token}@github.com/giankev/{repo_name}.git\"\n",
    "    os.system(f\"git clone {git_url} /kaggle/working/{repo_name}\")\n",
    "    %cd PDDLR-algorithm/\n",
    "\n",
    "import sys\n",
    "sys.path.append('./scr/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:58:25.679314Z",
     "iopub.status.busy": "2025-07-08T09:58:25.678965Z",
     "iopub.status.idle": "2025-07-08T09:58:33.772836Z",
     "shell.execute_reply": "2025-07-08T09:58:33.772311Z",
     "shell.execute_reply.started": "2025-07-08T09:58:25.679289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from tqdm import tqdm\n",
    "import gdown\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "from pdlpr import PDLPR \n",
    "\n",
    "from trainer import train, set_seed, evaluate_model\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:58:40.651157Z",
     "iopub.status.busy": "2025-07-08T09:58:40.650910Z",
     "iopub.status.idle": "2025-07-08T09:58:40.657936Z",
     "shell.execute_reply": "2025-07-08T09:58:40.657331Z",
     "shell.execute_reply.started": "2025-07-08T09:58:40.651140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\",\n",
    "             \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N',\n",
    "             'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R',\n",
    "       'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "unique_chars = set(provinces[:-1] + alphabets[:-1] + ads[:-1])  # escludi 'O'\n",
    "char_list = sorted(list(unique_chars))  # ordinamento per coerenza\n",
    "char_list = [\"-\"] + char_list\n",
    "char2idx = {char: i for i, char in enumerate(char_list)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "num_classes = len(char_list)\n",
    "print(\"Num classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding and encoding plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:58:44.873802Z",
     "iopub.status.busy": "2025-07-08T09:58:44.873276Z",
     "iopub.status.idle": "2025-07-08T09:58:44.885293Z",
     "shell.execute_reply": "2025-07-08T09:58:44.884607Z",
     "shell.execute_reply.started": "2025-07-08T09:58:44.873783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_plate(s):\n",
    "    idx   = list(map(int, s.split(\"_\")))\n",
    "    try:\n",
    "        return provinces[idx[0]] + alphabets[idx[1]] + \"\".join(ads[i] for i in idx[2:])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def decode_ccpd_label(label_str, provinces, alphabets, ads):\n",
    "    \"\"\"Decodifica stringa del tipo '0_0_22_27_27_33_16' in targa es. '皖AWWX6G' \"\"\"\n",
    "    indices = list(map(int, label_str.strip().split('_')))\n",
    "    if len(indices) != 7:\n",
    "        raise ValueError(\"Label must contain 7 indices\")\n",
    "\n",
    "    province = provinces[indices[0]]\n",
    "    alphabet = alphabets[indices[1]]\n",
    "    ad_chars = [ads[i] for i in indices[2:]]\n",
    "\n",
    "    return province + alphabet + ''.join(ad_chars)\n",
    "\n",
    "def encode_plate(plate_str, char2idx):\n",
    "    \"\"\"Converte la stringa '皖AWWX6G' in lista di indici [3, 12, 30, 30, ...]\"\"\"\n",
    "    return [char2idx[c] for c in plate_str]\n",
    "\n",
    "def decode_plate_from_list(label_indices, idx2char):\n",
    "    \"\"\"Converte una lista di indici [3, 12, 30, ...] nella stringa '皖AWWX6G'\"\"\"\n",
    "    return ''.join([idx2char[i] for i in label_indices])\n",
    "\n",
    "def greedy_decode(logits, blank_index, idx2char):\n",
    "    preds = logits.argmax(dim=2)  # (B, T)\n",
    "    decoded_batch = []\n",
    "    for pred in preds:\n",
    "        chars = []\n",
    "        prev = None\n",
    "        for p in pred:\n",
    "            p = p.item()\n",
    "            if p != blank_index and p != prev:\n",
    "                chars.append(idx2char[p])\n",
    "            prev = p\n",
    "        decoded_batch.append(''.join(chars))\n",
    "    return decoded_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_bbox(bbox_str):\n",
    "    # Split on one or more underscores\n",
    "    tokens = re.split(r'_+', bbox_str)\n",
    "    if len(tokens) == 4 and all(t.isdigit() for t in tokens):\n",
    "        return tuple(map(int, tokens))\n",
    "    return (None,) * 4\n",
    "\n",
    "\n",
    "def crop_and_resize(img, x1, y1, x2, y2):\n",
    "    # Controlla che il bounding box sia valido\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    \n",
    "    # Ritaglia\n",
    "    cropped_img = img[y1:y2, x1:x2]\n",
    "\n",
    "    # Controlla che l'immagine ritagliata non sia vuota\n",
    "    if cropped_img.size == 0:\n",
    "        return None\n",
    "\n",
    "    # Resize a 48x144\n",
    "    try:\n",
    "        return cv2.resize(cropped_img, (144, 48))\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_and_extract_dataset(url, output_path, extract_path, extracted_folder_path):\n",
    "    \"\"\"\n",
    "    Downloads and extracts a dataset if not already present.\n",
    "\n",
    "    Args:\n",
    "        url (str): Google Drive URL of the dataset.\n",
    "        output_path (str): Path where the .tar file will be saved.\n",
    "        extract_path (str): Directory where the archive will be extracted.\n",
    "        extracted_folder_path (str): Expected folder resulting from extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the dataset if it doesn't already exist\n",
    "    if not os.path.exists(output_path):\n",
    "        print(\"Downloading the dataset...\")\n",
    "        gdown.download(url, output_path, fuzzy=True, quiet=False)\n",
    "    else:\n",
    "        print(\"Dataset already exists, download skipped.\")\n",
    "\n",
    "    # Extract the dataset if the folder doesn't already exist\n",
    "    if not os.path.exists(extracted_folder_path):\n",
    "        print(\"Extracting the dataset...\")\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "        with tarfile.open(output_path) as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        print(\"Extraction completed.\")\n",
    "    else:\n",
    "        print(\"Dataset folder already exists, extraction skipped.\")\n",
    "\n",
    "\n",
    "def create_dataframe(folder_path, char2idx):\n",
    "    all_files = sorted(os.listdir(folder_path))\n",
    "    jpg_files = [f for f in all_files if f.endswith('.jpg')]\n",
    "\n",
    "    rows = []\n",
    "    for fname in jpg_files:\n",
    "        parts = fname[:-4].split(\"-\")\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            x1, y1, x2, y2 = split_bbox(parts[2])\n",
    "            plate = decode_plate(parts[4])\n",
    "            label = encode_plate(plate, char2idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore con file {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"image_path\": os.path.join(folder_path, fname),\n",
    "            \"x1_bbox\": x1, \"y1_bbox\": y1,\n",
    "            \"x2_bbox\": x2, \"y2_bbox\": y2,\n",
    "            \"plate_number\": plate,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def create_cropped_dataframe_inference(df, cropped_folder):\n",
    "    \"\"\"\n",
    "    Crea un nuovo DataFrame con le immagini ritagliate e ridimensionate.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame originale con bounding box e info.\n",
    "        cropped_folder (str): Cartella dove salvare le immagini ritagliate.\n",
    "        crop_and_resize_fn (function): Funzione che riceve (img, x1, y1, x2, y2) e restituisce l'immagine ritagliata e ridimensionata.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Nuovo DataFrame con path immagini ritagliate, plate_number e label.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(cropped_folder, exist_ok=True)\n",
    "    cropped_rows = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_path = row[\"image_path\"]\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Immagine non trovata o corrotta: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            x1 = int(float(row[\"x1_pred\"]))\n",
    "            y1 = int(float(row[\"y1_pred\"]))\n",
    "            x2 = int(float(row[\"x2_pred\"]))\n",
    "            y2 = int(float(row[\"y2_pred\"]))\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nei bounding box per {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        resized_img = crop_and_resize(img, x1, y1, x2, y2)\n",
    "        if resized_img is None:\n",
    "            print(f\"Errore nel crop/resize dell'immagine: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        cropped_path = os.path.join(cropped_folder, f\"cropped_{i}.jpg\")\n",
    "        cv2.imwrite(cropped_path, resized_img)\n",
    "\n",
    "        cropped_rows.append({\n",
    "            \"image_path\": cropped_path,\n",
    "            \"plate_number\": row[\"plate_number\"],\n",
    "            \"label\": row[\"label\"]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(cropped_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_and_evaluate(model, image_tensor, target_indices, char2idx, idx2char, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Assumiamo batch_size = 1\n",
    "    images = image_tensor.unsqueeze(0).to(device)       # (1, C, H, W)\n",
    "    targets = [target_indices.to(device)]               # list of tensors\n",
    "    target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long, device=device)\n",
    "    targets_concat = torch.cat(targets)                 # flatten targets\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(images)                              # (1, T, C)\n",
    "\n",
    "    # Decoding (greedy)\n",
    "    blank_idx = char2idx['-']\n",
    "    decoded = greedy_decode(logits, blank_idx, idx2char)\n",
    "\n",
    "    # Prepare input for CTC loss\n",
    "    log_probs = F.log_softmax(logits, dim=2).permute(1, 0, 2)  # (T, N, C)\n",
    "    input_lengths = torch.full(size=(1,), fill_value=log_probs.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "    # CTC Loss\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=blank_idx, zero_infinity=True)\n",
    "    loss = ctc_loss_fn(log_probs, targets_concat, input_lengths, target_lengths)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Predetta: {decoded[0]}\")\n",
    "    print(f\"Target:   {decode_plate_from_list(target_indices.tolist(), idx2char)}\")\n",
    "    print(f\"CTC Loss: {loss.item():.4f}\")\n",
    "    print(f\"Len pred: {len(decoded[0])}, Len true: {target_lengths.item()}\")\n",
    "\n",
    "    return decoded[0], loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:46:06.111597Z",
     "iopub.status.busy": "2025-07-08T09:46:06.110946Z",
     "iopub.status.idle": "2025-07-08T09:46:14.731406Z",
     "shell.execute_reply": "2025-07-08T09:46:14.730458Z",
     "shell.execute_reply.started": "2025-07-08T09:46:06.111573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS = 0\n",
    "SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 5\n",
    "set_seed(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pdlpr_checkpoint_path = \"pdlpr_checkpoints/checkpoint_epoch30.pt\"\n",
    "yolo_checkpoint_path = \"\"\n",
    "\n",
    "test_output_path = 'dataset/ccpd_test.tar'\n",
    "test_extract_path = 'dataset'\n",
    "test_folder_path = os.path.join(test_extract_path, 'ccpd_test')\n",
    "test_cropped_folder = 'dataset/ccpd_test_cropped'\n",
    "\n",
    "# Adatta i percorsi se stai lavorando su Kaggle\n",
    "if working_on_kaggle:\n",
    "    NUM_WORKERS = 2\n",
    "    pdlpr_checkpoint_path = '/kaggle/working/' + pdlpr_checkpoint_path\n",
    "    test_output_path = '/kaggle/working/' + test_output_path\n",
    "    test_extract_path = '/kaggle/working/' + test_extract_path\n",
    "    test_folder_path = os.path.join(test_extract_path, 'ccpd_test')\n",
    "    test_cropped_folder = '/kaggle/working/' + test_cropped_folder\n",
    "\n",
    "# Crea cartelle se non esistono\n",
    "os.makedirs(os.path.dirname(test_output_path), exist_ok=True)\n",
    "os.makedirs(test_cropped_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "if yolo_checkpoint_path is not None and not os.path.isfile(yolo_checkpoint_path):\n",
    "    raise FileNotFoundError(f\"YOLOvs checkpoint file not found: {yolo_checkpoint_path}\")\n",
    "\n",
    "if pdlpr_checkpoint_path is not None and not os.path.isfile(pdlpr_checkpoint_path):\n",
    "    raise FileNotFoundError(f\"PDLPR checkpoint file not found: {pdlpr_checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL_TEST di download\n",
    "file_id_test = '1PnYtN0P6m36LmjztvhVmVLqZwZAp9Q3X'\n",
    "url_test = f'https://drive.google.com/uc?id={file_id_test}'\n",
    "\n",
    "download_and_extract_dataset(url_test, test_output_path, test_extract_path, test_folder_path)\n",
    "\n",
    "if os.path.exists(test_folder_path):\n",
    "    subfolders = [name for name in os.listdir(test_folder_path)\n",
    "                  if os.path.isdir(os.path.join(test_folder_path, name))]\n",
    "    subfolders = sorted(subfolders)\n",
    "    print(f\"Subfolders in '{test_folder_path}':\")\n",
    "    # for folder in subfolders:\n",
    "    #     print(f\"- {folder}\")\n",
    "else:\n",
    "    print(f\"The folder '{test_folder_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo normalizzazione\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 144)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset personalizzato\n",
    "class PlateDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = row[\"label\"]  # list\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: carica il modello YOLO\n",
    "\n",
    "\n",
    "\n",
    "# TODO: carica il modello pdlpr. \n",
    "pdlpr_model = PDLPR().to(device)\n",
    "pdlpr_model.eval()  \n",
    "\n",
    "if pdlpr_checkpoint_path and os.path.isfile(pdlpr_checkpoint_path):\n",
    "    checkpoint = torch.load(pdlpr_checkpoint_path, map_location=device)\n",
    "    pdlpr_model.load_state_dict(checkpoint['weights'])\n",
    "    print(f\"Checkpoint caricato da {pdlpr_checkpoint_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint non trovato: {pdlpr_checkpoint_path}\")\n",
    "\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    print(f\"\\nEvaluation on CCPD_{subfolder}\")\n",
    "    subfolder_path = os.path.join(test_folder_path, subfolder)\n",
    "    sub_df = create_dataframe(subfolder_path, char2idx)\n",
    "\n",
    "    # TODO: preprocessing immagini per YOLO \n",
    "    \n",
    "    # TODO: fare prediction con YOLO\n",
    "\n",
    "    # TODO. salvare i bounding box come colonne di sub_df. Le colonne si devono chiamare x1_pred, y1_pred, x2_pred, y2_pred \n",
    "\n",
    "\n",
    "    # Creazione delle immagini croppate\n",
    "    cropped_subfolder =  os.path.join(test_cropped_folder, subfolder)\n",
    "    os.makedirs(cropped_subfolder, exist_ok=True)\n",
    "    # Creazione delle dataset con le immagini croppate\n",
    "    cropped_sub_df = create_cropped_dataframe_inference(sub_df, cropped_subfolder)\n",
    "    # Creazione del dataloader per PDLPR model\n",
    "    test_dataset = PlateDataset(cropped_sub_df, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    test_loss, test_char_acc, test_seq_acc = evaluate_model(pdlpr_model, test_loader, char2idx, \"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
